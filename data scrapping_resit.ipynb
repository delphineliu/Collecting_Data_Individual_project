{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef9f1c7",
   "metadata": {},
   "source": [
    "###  Collecting reviews of a book can be found can be interesting .Through these reviews crawling and simple data analysis we can have a intuitive understanding the trend of readers' evaluation of the hot book â€œSpare\" by prince Harry and in-depth discussion of the issue. This program is divided into two parts, this first part is data crawling, crawled data has been stored as csv which can be seen on my github. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7196a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab50bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The headers dictionary contains the user agent and cookie information that is passed along with the request.\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61\",\n",
    "    \"cookie\": 'session-id=259-7092877-1329027; session-id-time=2082787201l; i18n-prefs=EUR; lc-acbnl=en_GB; ubid-acbnl=261-7387351-3032447; session-token=\"G+snIb/elscOZey+wXBB4W0YBYC3B0lJ0mt/o58U+ULz+ACblrt0Atzm3KgB3vYSYXmeeeekuACBfBWGtAIWS2+t91HSX9fXeBhCLFa6t6w+TXhDKJVWrbmuqQnhVVjbDQKQ3fHX/hWbOeC3g4jBbZ9ZlUL0ideahso2zao2JP+Eo6R5aFhQ9PG8LKBnQLUHqii4HqRF2yITBZoFkdzUwolOjxBRDiAF0YUyGQRkFuQ=\"; csm-hit=tb:XH02Q09KM4609746M2ED+s-81C215BMAMGZBCB7VN7G|1674460277644&t:1674460277644&adb:adblk_no'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc91515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(url):\n",
    "    x = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(x.text, 'lxml')\n",
    "    one_page_data = soup.find_all(class_='a-section review aok-relative')\n",
    "    return one_page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145b25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.nl/-/en/gp/aw/reviews/0857504797/ref=cm_cr_getr_mb_paging_btm_5?ie=UTF8&pageNumber=1\n"
     ]
    }
   ],
   "source": [
    "# Amazon reviews can be seen in the URL, we can crawl all the reviews using the modification of the review page number. \n",
    "# The all_reviews list is initialized to an empty list, and a for loop is used to iterate through each page of reviews.\n",
    "\n",
    "# We can see through the crawling process that there are 10 reviews per page.\n",
    "# Then based on the analysis of the crawled page, beautifulsoup is used to find the text part of the information to be crawled in the tags\n",
    "# and add it to the all_reviews list one by one\n",
    "all_reviews = []\n",
    "for page in range(1,277):\n",
    "    url = f'https://www.amazon.nl/-/en/gp/aw/reviews/0857504797/ref=cm_cr_getr_mb_paging_btm_5?ie=UTF8&pageNumber={page}'\n",
    "    one_page_data = get_data(url)\n",
    "    if len(one_page_data) == 0:\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    "            one_page_data = get_data(url)\n",
    "            if len(one_page_data) != 0:\n",
    "                break\n",
    "# Considering the anti-crawling mechanism of Amazon.com, we can  disguise user-agent information. \n",
    "# in the process of crawling we may encounter the request times out or fails, \n",
    "# then we can make it wait 5 seconds before trying again until the data has been crawled.\n",
    "\n",
    "        print(url)\n",
    "    else:\n",
    "        print(url)\n",
    "    one_page_reviews = []\n",
    "    for a in one_page_data:\n",
    "        name = a.find(class_='a-profile-name').text\n",
    "        date = a.find(attrs={\"data-hook\": \"review-date\"}).text\n",
    "        rating = a.find(class_='a-icon-alt').text\n",
    "        title = a.find(attrs={\"data-hook\": \"review-title\"}).text[1:-1]\n",
    "        review = a.find(attrs={\"data-hook\": \"review-body\"}).text[1:-1]\n",
    "        one_page_reviews.append([name,date,rating,title,review])\n",
    "    all_reviews += one_page_reviews\n",
    "    page += 1\n",
    "    \n",
    "# The name, date, rating, title, and review information for each review is extracted using find() and attrs() methods on the soup object, \n",
    "# and is appended to the one_page_reviews list.\n",
    "# The one_page_reviews list is then appended to the all_reviews list, and the loop continues to the next page.\n",
    "###THIS PROCESSING WILL TAKE A WHILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_reviews, columns=['name','date','rating','title','review'])\n",
    "data.to_csv('reviews.csv', index=False)\n",
    "# store the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
